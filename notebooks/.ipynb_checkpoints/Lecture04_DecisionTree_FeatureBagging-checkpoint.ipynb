{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92036db1",
   "metadata": {},
   "source": [
    "# Lecture 04 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b4ef2",
   "metadata": {},
   "source": [
    "### 1 Full Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cef38",
   "metadata": {},
   "source": [
    "A Decision Tree works by recursively splitting the data based on the best feature and threshold to minimize some impurity (e.g., Gini impurity or entropy). This process continues until certain stopping conditions are met (e.g., reaching a maximum depth, or a node having a minimum number of samples).\n",
    "\n",
    "In this implementation, we'll use:\n",
    "- Gini Impurity as the splitting criterion.\n",
    "- Stopping conditions: maximum depth or minimum samples per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79548929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Utility function to calculate Gini Impurity\n",
    "def gini(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    return 1 - np.sum(p ** 2)\n",
    "\n",
    "# Utility function to split the dataset\n",
    "def split(X, y, feature_index, threshold):\n",
    "    left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "    right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf025fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Node (Recursive Structure)\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # Index of the feature to split on\n",
    "        self.threshold = threshold  # Threshold value for the split\n",
    "        self.left = left  # Left subtree\n",
    "        self.right = right  # Right subtree\n",
    "        self.value = value  # Class label for leaf nodes\n",
    "\n",
    "# Full Decision Tree Classifier\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_feature, best_threshold, best_gini = None, None, float('inf')\n",
    "        best_splits = None\n",
    "\n",
    "        # Try splitting on each feature and each unique threshold\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = split(X, y, feature_index, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate weighted Gini Impurity for the split\n",
    "                gini_left, gini_right = gini(y_left), gini(y_right)\n",
    "                gini_split = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\n",
    "\n",
    "                if gini_split < best_gini:\n",
    "                    best_gini = gini_split\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_splits = (X_left, X_right, y_left, y_right)\n",
    "\n",
    "        return best_feature, best_threshold, best_splits\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "\n",
    "        # Stopping conditions\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split or n_classes == 1):\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "\n",
    "        # Find the best feature and threshold to split\n",
    "        best_feature, best_threshold, best_splits = self._best_split(X, y)\n",
    "        if best_splits is None:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return DecisionTreeNode(value=leaf_value)\n",
    "\n",
    "        X_left, X_right, y_left, y_right = best_splits\n",
    "\n",
    "        # Recursively build left and right subtrees\n",
    "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return DecisionTreeNode(best_feature, best_threshold, left_child, right_child)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _predict(self, node, X):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        feature_value = X[node.feature_index]\n",
    "        if feature_value <= node.threshold:\n",
    "            return self._predict(node.left, X)\n",
    "        else:\n",
    "            return self._predict(node.right, X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(self.root, x) for x in X])\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[2, 3], [10, 15], [5, 8], [6, 9], [7, 10], [3, 5]])\n",
    "    y = np.array([0, 1, 0, 0, 1, 1])\n",
    "\n",
    "    # Instantiate and train the decision tree\n",
    "    clf = DecisionTree(max_depth=3, min_samples_split=2)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = clf.predict(X)\n",
    "    print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f252656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with Feature Bagging: [0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# reduce the max_depth = 1 and see what happens\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[2, 3], [10, 15], [5, 8], [6, 9], [7, 10], [3, 5]])\n",
    "    y = np.array([0, 1, 0, 0, 1, 1])\n",
    "\n",
    "    # Instantiate and train the decision tree with feature bagging\n",
    "    clf = DecisionTreeWithFeatureBagging(max_depth=1, min_samples_split=2, max_features=1)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = clf.predict(X)\n",
    "    print(\"Predictions with Feature Bagging:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fe72b",
   "metadata": {},
   "source": [
    "Key Steps:\n",
    "\n",
    "- Gini Impurity is used to evaluate how \"pure\" a node is after the split. The lower the Gini impurity, the better the split.\n",
    "- Recursive Tree Building: The _build_tree method creates a recursive structure for the decision tree. It stops splitting when the tree reaches the maximum depth, or if the number of samples or class diversity falls below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf1b40",
   "metadata": {},
   "source": [
    "### 2 Feature Bagging (One Branch in Random Forest)\n",
    "#### use random subset of features per split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b67a48",
   "metadata": {},
   "source": [
    "In Feature Bagging, instead of considering all features when splitting a node, a random subset of features is chosen. This helps decorrelate the trees in the random forest, making the model more robust. Here's how you can integrate feature bagging into a decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e11b1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions with Feature Bagging: [0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeWithFeatureBagging(DecisionTree):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        super().__init__(max_depth, min_samples_split)\n",
    "        self.max_features = max_features  # Max number of features to consider for each split\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_feature, best_threshold, best_gini = None, None, float('inf')\n",
    "        best_splits = None\n",
    "\n",
    "        # Select random subset of features\n",
    "        feature_indices = np.random.choice(n_features, self.max_features, replace=False)\n",
    "\n",
    "        # Try splitting on each feature and each unique threshold\n",
    "        for feature_index in feature_indices:\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = split(X, y, feature_index, threshold)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate weighted Gini Impurity for the split\n",
    "                gini_left, gini_right = gini(y_left), gini(y_right)\n",
    "                gini_split = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right\n",
    "\n",
    "                if gini_split < best_gini:\n",
    "                    best_gini = gini_split\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_splits = (X_left, X_right, y_left, y_right)\n",
    "\n",
    "        return best_feature, best_threshold, best_splits\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data\n",
    "    X = np.array([[2, 3, 7], [10, 15, 11], [5, 8, 4], [6, 9, 7], [7, 10, 12], [3, 5, 11]])\n",
    "    y = np.array([0, 1, 0, 0, 1, 1])\n",
    "\n",
    "    # Instantiate and train the decision tree with feature bagging\n",
    "    clf = DecisionTreeWithFeatureBagging(max_depth=3, min_samples_split=2, max_features=2)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = clf.predict(X)\n",
    "    print(\"Predictions with Feature Bagging:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fec156",
   "metadata": {},
   "source": [
    "Key Steps:\n",
    "- Feature Bagging: Instead of considering all features at each split, the code selects a random subset of features. The number of features to consider is defined by max_features.\n",
    "- In a Random Forest, each tree uses feature bagging, making the trees diverse and reducing correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df407e69",
   "metadata": {},
   "source": [
    "Now you have:\n",
    "- A full Decision Tree implementation with recursive splits based on Gini impurity.\n",
    "- Feature Bagging, where a random subset of features is used for splitting, integrated into the decision tree.\n",
    "- These components are foundational to building a robust Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8399f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
